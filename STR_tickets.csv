ticket_id,title,type,priority,summary,acceptance_criteria,dependencies,estimated_effort,labels,status
STR-001,"Set up Python project structure, dependencies, and config module",Task,Critical,"Create the foundational project structure as defined in the spec (src/, tests/, data/, assets/ directories). Set up requirements.txt with all dependencies (praw, pytrends, openai, replicate, pillow, supabase-py, boto3, redis, sendgrid, pytest). Implement src/config.py to load all environment variables with validation and defaults. Create .env.example with all required keys documented.","1. Project follows the directory structure defined in the spec (src/trends/, src/stickers/, src/pricing/, src/moderation/, src/publisher/, src/fulfillment/, src/analytics/, src/monitoring/, src/backup/, tests/unit/, tests/integration/, tests/e2e/, data/, assets/mockups/); 2. requirements.txt includes all necessary packages (praw, pytrends, openai, replicate, pillow, supabase, boto3, redis, sendgrid, pytest) with pinned versions; 3. src/config.py loads all environment variables listed in the spec and raises clear errors if required variables are missing; 4. .env.example documents every variable with comments explaining its purpose; 5. Python 3.12 is specified and the project runs without import errors; 6. A basic __init__.py exists in each package directory",None,S,"backend,infrastructure,setup",To Do
STR-002,Create Supabase database schema and migrations,Task,Critical,"Create all database tables defined in the spec: trends, stickers, orders, pipeline_runs, error_log, etsy_tokens, pricing_tiers, shipping_rates, price_history. Create all indexes. Create the daily_metrics materialized view and cost_tracking view. Enable Row-Level Security on all tables. Seed pricing_tiers and shipping_rates with default data from the spec.","1. All 9 tables (trends, stickers, orders, pipeline_runs, error_log, etsy_tokens, pricing_tiers, shipping_rates, price_history) are created with exact column definitions matching the spec; 2. All indexes defined in the spec are created (idx_trends_status, idx_trends_topic_normalized, idx_trends_created, idx_stickers_pricing_tier, idx_stickers_trend_id, idx_stickers_published, idx_stickers_moderation, idx_orders_status, idx_orders_sticker, idx_orders_created, idx_pipeline_runs_workflow, idx_pipeline_runs_status, idx_pipeline_runs_started, idx_error_log_workflow, idx_error_log_created, idx_error_log_unresolved); 3. daily_metrics materialized view and cost_tracking view are created per spec SQL; 4. RLS is enabled on all tables per spec; 5. pricing_tiers table is seeded with 4 tiers (just_dropped, trending, cooling, evergreen) with correct prices; 6. shipping_rates table is seeded with 4 rows (sticker_mule and self_usps for single_small and single_large) with correct costs; 7. Migration scripts are idempotent (can be re-run safely)",None,M,"database,infrastructure,setup",To Do
STR-003,Implement Supabase client wrapper (src/db.py),Task,Critical,"Implement the Supabase client module that initializes the Supabase connection using the service role key. Provide helper methods for common CRUD operations on all tables (trends, stickers, orders, etc.). Include connection error handling and retry logic. This module is used by every other component in the system.","1. Supabase client initializes using SUPABASE_URL and SUPABASE_SERVICE_KEY from environment; 2. GIVEN missing or invalid Supabase credentials WHEN the client initializes THEN a clear error is raised with instructions; 3. Helper methods exist for insert, update, select, and delete on all major tables; 4. Connection errors are caught and logged with descriptive messages; 5. The client supports row-level locking for etsy_tokens table (concurrent token refresh); 6. Unit tests verify CRUD operations work against a test Supabase instance",STR-001,S,"backend,database",To Do
STR-004,Implement Cloudflare R2 storage client (src/publisher/storage.py),Task,Critical,"Implement the R2 storage client using boto3 (S3-compatible API). Support uploading sticker images (original, print-ready, thumbnail), database backups, and listing/deleting objects. Configure public read access for sticker images. Set appropriate content types and cache headers.","1. R2 client initializes with CLOUDFLARE_R2_ACCESS_KEY, CLOUDFLARE_R2_SECRET_KEY, CLOUDFLARE_R2_BUCKET, and CLOUDFLARE_R2_ENDPOINT; 2. upload_image() accepts a file path or bytes and a key, uploads to R2, and returns the public URL; 3. Images are uploaded with Content-Type: image/png and appropriate cache headers; 4. upload_backup() uploads gzipped database backups to the backups/db/ prefix; 5. delete_object() removes an object by key; 6. list_objects() lists objects by prefix (used for backup cleanup); 7. GIVEN R2 is unreachable WHEN an upload is attempted THEN the error is caught, logged, and raised for retry handling; 8. Unit tests verify upload/download/delete with mocked boto3",STR-001,S,"backend,storage",To Do
STR-005,Implement resilience module with retry decorator and circuit breaker (src/resilience.py),Task,Critical,"Implement the shared resilience utilities used by all external API calls: a retry decorator with configurable exponential backoff (base=2, max=30 seconds, max_retries=3), and a circuit breaker that trips after N consecutive failures per service. The circuit breaker thresholds are defined per-service in the spec (5 for Reddit/Google/OpenAI, 3 for Replicate/Etsy/Supabase/R2).","1. @retry decorator supports max_retries, backoff_base, and backoff_max parameters with defaults matching the spec (max_retries=3, backoff_base=2, backoff_max=30); 2. Retry wait times follow exponential backoff: 2s, 4s, 8s (capped at 30s); 3. Circuit breaker tracks consecutive failures per service name; 4. GIVEN 5 consecutive failures to Reddit API WHEN the next call is attempted THEN the circuit breaker skips the call and logs the skip; 5. GIVEN 3 consecutive failures to Replicate API WHEN the next call is attempted THEN the circuit breaker skips the call; 6. Circuit breaker state resets between workflow runs (not persisted across runs); 7. Both retry and circuit breaker log each attempt with service name, attempt number, and error details; 8. Unit tests verify backoff timing, circuit breaker trip/reset, and interaction between retry and circuit breaker","STR-001,STR-003",M,"backend,infrastructure",To Do
STR-006,Implement pipeline run logger (src/monitoring/pipeline_logger.py),Task,High,"Implement the pipeline run logging module that records every workflow execution to the pipeline_runs table. Each run tracks: workflow name, status (started/completed/failed/partial), timing, counts (trends found, stickers generated, published, archived, prices updated, orders synced, errors), Etsy API calls consumed, and estimated AI cost.","1. start_run() creates a new pipeline_runs row with status='started' and started_at=now(), returns the run ID; 2. complete_run() updates the row with status='completed', ended_at, duration_seconds, and all count fields; 3. fail_run() updates the row with status='failed' and error details in metadata; 4. Supports partial completion status for runs where some items succeed and others fail; 5. ai_cost_estimate_usd is calculated from token counts (GPT-4o-mini) and image counts (Replicate); 6. GIVEN a workflow run completes WHEN the logger writes the record THEN all fields match the actual execution metrics; 7. Unit tests verify status transitions and field population","STR-001,STR-003",S,"backend,monitoring",To Do
STR-007,Implement error logger (src/monitoring/error_logger.py),Task,High,"Implement the error logging module that records errors to the error_log table. Each error includes: pipeline_run_id, workflow, step, error_type, error_message, service, retry_count, and context (JSONB for trend_id, sticker_id, etc.). Must never log PII or API keys per the security requirements.","1. log_error() writes to error_log with all required fields (pipeline_run_id, workflow, step, error_type, error_message, service, retry_count, context); 2. Context JSONB never contains PII (customer names, addresses, emails) or API keys; 3. Error messages are sanitized to strip any accidentally included API keys or tokens; 4. GIVEN a scoring error occurs for trend_id=abc WHEN log_error is called THEN the error_log row contains context={""trend_id"": ""abc""} without any PII; 5. Supports marking errors as resolved (resolved=true); 6. Query helper to check for 3+ consecutive failures of the same workflow (used for GitHub Actions monitoring alerts); 7. Unit tests verify sanitization strips sensitive data","STR-001,STR-003",S,"backend,monitoring",To Do
STR-008,Implement email alerter (src/monitoring/alerter.py),Task,High,"Implement the SendGrid email alert module for sending operational alerts and daily summary emails. Supports: critical alerts (OAuth failure, all APIs down, DB down), warning alerts (rate limits approaching, high error rate), and daily summary emails. Email content must never include PII per security requirements.","1. send_alert() sends an email via SendGrid to the ALERT_EMAIL address with subject and body; 2. send_daily_summary() sends the daily summary email formatted per the spec template (pipeline health, revenue, pricing, costs, alerts sections); 3. GIVEN SendGrid API key is configured WHEN send_alert is called THEN an email is delivered with correct subject and body; 4. Alert emails never contain PII (customer data, full API responses, tokens); 5. GIVEN SendGrid is unreachable WHEN an alert is attempted THEN the failure is logged to error_log and the workflow continues (alerts are best-effort); 6. send_moderation_alert() sends flagged sticker details (image URL, topic, scores) for manual review; 7. Unit tests verify email formatting and PII exclusion with mocked SendGrid",STR-001,S,"backend,monitoring,email",To Do
STR-009,Implement Upstash Redis client and Etsy API rate limiter (src/publisher/etsy_rate_limiter.py),Task,High,"Implement the Redis-based Etsy API rate limit tracker using Upstash Redis. Track daily API call counts with key format etsy_api_calls:{YYYY-MM-DD} and TTL of 48 hours. Implement priority-based throttling: Normal (0-7000), Warning (7001-8500, skip P3), Critical (8501-9500, only P0/P1), Hard stop (9501+, halt all). Also implement concurrency locks for workflow dedup.","1. increment_api_calls() atomically increments the daily counter and returns the new count; 2. get_daily_usage() returns the current day's API call count; 3. can_proceed(priority) returns true/false based on current usage level and operation priority (P0-P3); 4. GIVEN daily calls exceed 7000 WHEN a P3 (analytics) operation calls can_proceed(3) THEN it returns false and the skip is logged; 5. GIVEN daily calls exceed 9500 WHEN any operation calls can_proceed() THEN it returns false and an alert email is triggered; 6. acquire_lock(workflow_name, ttl) acquires a Redis lock and returns true/false; 7. release_lock(workflow_name) releases the lock; 8. Lock TTLs match spec: trend_monitor=25min, pricing_engine=30min, analytics_sync=30min; 9. GIVEN a lock is held WHEN another workflow tries to acquire it THEN it returns false and the workflow exits gracefully; 10. Daily counter key has 48-hour TTL for auto-cleanup; 11. Unit tests verify threshold logic and lock behavior with mocked Redis","STR-001,STR-005",M,"backend,cache,infrastructure",To Do
STR-010,Implement Etsy OAuth token manager (src/publisher/etsy_auth.py),Task,Critical,"Implement the Etsy OAuth 2.0 token management module. Read tokens from Supabase etsy_tokens table. Auto-refresh access tokens that expire within 5 minutes. Handle concurrent refresh (row locking). Alert and halt on invalid_grant errors. Store new tokens back to Supabase.","1. get_access_token(shop_id) reads the current token from Supabase and returns a valid access token; 2. GIVEN the access token expires within 5 minutes WHEN get_access_token is called THEN it refreshes the token via POST to https://api.etsy.com/v3/public/oauth/token with grant_type=refresh_token; 3. GIVEN a refresh succeeds WHEN new tokens are returned THEN access_token, refresh_token, and expires_at are all updated in Supabase; 4. GIVEN the refresh token is invalid (Etsy returns invalid_grant) WHEN refresh is attempted THEN an alert email is sent with subject containing 'manual re-authorization required' and all Etsy workflows are halted; 5. GIVEN two concurrent workflows try to refresh at the same time WHEN both call get_access_token THEN only one refresh occurs (row locking prevents race condition) and the second workflow uses the newly refreshed token; 6. Token refresh uses ETSY_API_KEY as client_id per the spec; 7. Unit tests verify refresh logic, expiry detection, and error handling with mocked Etsy API","STR-003,STR-005,STR-008",M,"backend,auth,etsy",To Do
STR-011,Implement Reddit trend source (src/trends/sources/reddit.py),Task,High,"Implement the Reddit OAuth API client for fetching rising/trending posts. Monitor subreddits: r/memes, r/funny, r/trending, and configurable niche subs. Authenticate via Reddit OAuth (PRAW or direct HTTP). Respect 60 req/min rate limit. Extract keywords, hashtags, and topics from post titles and content.","1. RedditSource.fetch_trends() authenticates via Reddit OAuth and returns a list of trending topics with keywords and sample post data; 2. Monitors r/memes, r/funny, r/trending, and additional configurable subreddits; 3. GIVEN valid Reddit OAuth credentials WHEN fetch_trends runs THEN it returns structured data with topic, keywords, and source_data for each trending post; 4. GIVEN Reddit API returns an error WHEN fetch_trends runs THEN the error is caught, logged to error_log, and an empty list is returned (graceful degradation); 5. GIVEN Reddit API is rate-limited WHEN requests exceed 60/min THEN the client backs off appropriately; 6. Extracts keywords from post titles using basic NLP (lowercase, remove stopwords, stem); 7. Returns source='reddit' in each result for cross-source tracking; 8. Integration test verifies real Reddit API returns parseable data","STR-001,STR-005",M,"backend,trends,api",To Do
STR-012,Implement Google Trends source (src/trends/sources/google_trends.py),Task,High,"Implement the Google Trends data source using pytrends library. Fetch breakout search terms. Rate-limit to 5 requests per cycle to avoid IP blocks. Extract trending search terms with their relative interest scores.","1. GoogleTrendsSource.fetch_trends() returns a list of breakout trending terms with relative interest scores; 2. Requests are rate-limited to max 5 per cycle; 3. GIVEN pytrends returns data WHEN fetch_trends runs THEN each result contains topic, keywords, and source_data fields; 4. GIVEN Google Trends is unreachable or rate-limited WHEN fetch_trends runs THEN the error is caught, logged, and an empty list is returned (graceful degradation); 5. Returns source='google_trends' in each result; 6. Integration test verifies pytrends returns parseable breakout terms; 7. No authentication is required (pytrends is unofficial/free)","STR-001,STR-005",S,"backend,trends,api",To Do
STR-013,Implement cross-source trend deduplication (src/trends/dedup.py),Task,High,"Implement trend deduplication using keyword normalization and Jaccard similarity. Normalize topics (lowercase, stem keywords). Compare incoming trends against each other and against existing trends in Supabase. Merge trends with Jaccard similarity > 0.6 into a canonical trend with a combined sources array.","1. normalize_topic(topic) returns a lowercase, stemmed version of the topic for matching; 2. jaccard_similarity(set_a, set_b) correctly calculates the Jaccard index between two keyword sets; 3. GIVEN two trends with Jaccard similarity > 0.6 WHEN dedup runs THEN they are merged into a single canonical trend with both sources in the sources array; 4. GIVEN two trends with Jaccard similarity <= 0.6 WHEN dedup runs THEN they remain as separate trends; 5. GIVEN a trend already exists in Supabase (matched by normalized topic) WHEN the monitor encounters it from a different source THEN the existing trend's sources array is updated without creating a duplicate row; 6. Dedup handles edge cases: empty keyword sets, single-word topics, very long topic strings; 7. Unit tests verify Jaccard calculation, normalization, merge logic, and edge cases","STR-001,STR-003",S,"backend,trends",To Do
STR-014,Implement GPT-4o-mini trend scorer (src/trends/scorer.py),Task,High,"Implement the trend scoring module using GPT-4o-mini with structured output (response_format=json_object). Score trends on 4 dimensions: velocity (1-10), commercial viability (1-10), content safety (1-10), uniqueness (1-10), plus overall composite score (1.0-10.0) and reasoning. Use the exact prompt template from the spec with calibration examples. Retry on malformed JSON (max 2 retries). Only pass trends scoring 7.0+ overall.","1. score_trend(topic, sample_posts, source_list) calls GPT-4o-mini and returns a structured score object with velocity, commercial, safety, uniqueness, overall, and reasoning fields; 2. Uses response_format={'type': 'json_object'} for structured output; 3. Uses the exact system and user prompt from the spec including calibration examples (Moo Deng=9-10, Taylor Swift=6-7, Federal Reserve=3-4); 4. GIVEN GPT-4o-mini returns valid JSON WHEN the scorer parses it THEN all score fields are present and within valid ranges (integers 1-10 for dimensions, float 1.0-10.0 for overall); 5. GIVEN GPT-4o-mini returns malformed JSON WHEN the scorer parses it THEN it retries up to 2 times; if all fail, the trend is skipped and error is logged; 6. GIVEN a trend scores 7.0+ overall WHEN the scorer evaluates it THEN it is returned as qualifying; 7. GIVEN a trend scores below 7.0 WHEN the scorer evaluates it THEN it is filtered out; 8. Unit tests verify score parsing, threshold filtering, retry on malformed JSON, and prompt construction","STR-001,STR-005",M,"backend,trends,ai",To Do
STR-015,Implement trademark and keyword blocklists (src/moderation/blocklist.py and src/moderation/trademark_blocklist.py),Task,High,"Create the trademark blocklist file (data/trademark_blocklist.txt) with major brands, fictional characters, celebrities, and sports teams. Create the keyword blocklist file (data/keyword_blocklist.txt) with offensive/inappropriate terms. Implement the matching logic that checks trend topics and sticker tags against both lists. Auto-reject trademarked terms with reason 'trademark_blocked'.","1. data/trademark_blocklist.txt contains at least 100 entries covering major brands (Nike, Apple, Disney, etc.), fictional characters (Mickey Mouse, Pikachu, Spider-Man), celebrity names, and sports team names; 2. data/keyword_blocklist.txt contains offensive, inappropriate, and NSFW terms; 3. check_trademark(text) returns true if the text matches any entry in the trademark blocklist (case-insensitive, partial match on multi-word terms); 4. check_keywords(text) returns true if the text matches any entry in the keyword blocklist; 5. GIVEN a trend topic contains 'Mickey Mouse' WHEN check_trademark runs THEN it returns true with match details; 6. GIVEN a clean topic like 'baby hippo' WHEN check_trademark runs THEN it returns false; 7. Matching is case-insensitive and handles plurals/possessives; 8. Unit tests verify matching against known brands, clean topics, edge cases (partial matches, substrings)",STR-001,S,"backend,moderation,security",To Do
STR-016,Implement trend monitor orchestrator (src/trends/monitor.py),Task,Critical,"Implement the main trend monitor that orchestrates the 2-hour trend detection cycle. Fetches from Reddit and Google Trends, deduplicates, checks blocklists, scores with GPT-4o-mini, and stores qualifying trends in Supabase. Enforces per-cycle cap of 5 trends and daily cap of 30 scored trends. Outputs new_trends=true/false for GitHub Actions conditional job execution. Handles partial failures gracefully.","1. GIVEN the monitor runs WHEN it finds topics scoring >= 7.0 from Reddit or Google Trends THEN they are stored in Supabase with status='discovered', all score fields populated, and sources array listing contributing platforms; 2. GIVEN more than 5 qualifying trends in one cycle WHEN the monitor processes the top 5 by score THEN remaining trends are stored with status='queued' for the next cycle; 3. GIVEN all trend source APIs are unreachable WHEN the monitor runs THEN it logs errors to error_log, sends an alert email, and exits with non-zero exit code; 4. GIVEN one trend source fails but another succeeds WHEN the monitor runs THEN it continues with available sources and logs the failure; 5. GIVEN a trend topic matches the trademark blocklist WHEN the scorer evaluates it THEN it is auto-rejected with reason='trademark_blocked' and not stored; 6. The monitor sets GitHub Actions output new_trends=true if any trends with status='discovered' were created this cycle; 7. GIVEN no trends score 7.0+ WHEN the cycle completes THEN it logs 'No qualifying trends found' and exits with code 0; 8. GIVEN zero qualifying trends for 24+ consecutive hours WHEN the monitor checks THEN it sends an alert; 9. Daily cap: max 30 trends scored per day; 10. Acquires Redis concurrency lock before running; exits if lock is held","STR-003,STR-005,STR-009,STR-011,STR-012,STR-013,STR-014,STR-015,STR-006,STR-007",L,"backend,trends,pipeline",To Do
STR-017,Implement image prompt generator (src/stickers/prompt_generator.py),Task,High,"Implement the module that uses GPT-4o-mini to generate 3 image prompts per trend. Each prompt follows the sticker-optimized template from the spec: die-cut format, bold outlines, vibrant colors, transparent/white background, NO text/brand names/logos/characters. Use structured output for consistent prompt format.","1. generate_prompts(trend) calls GPT-4o-mini and returns exactly 3 image prompt strings per trend; 2. Each prompt includes the required style directives from the spec: die-cut vinyl sticker, bold black outlines, vibrant colors, white background, cartoon illustration style, no text/words/letters/brand names/logos; 3. Prompts incorporate the trend topic and context meaningfully; 4. GIVEN a trend about 'baby hippo' WHEN prompts are generated THEN each prompt describes a unique visual interpretation suitable for a sticker; 5. GIVEN GPT-4o-mini fails WHEN prompt generation is attempted THEN it retries per the resilience policy and logs the error; 6. Uses response_format for structured output to ensure exactly 3 prompts are returned; 7. Unit tests verify prompt count, required style keywords, and error handling","STR-001,STR-005",S,"backend,stickers,ai",To Do
STR-018,Implement Replicate SDXL image generator (src/stickers/image_generator.py),Task,Critical,"Implement the image generation module using Replicate's Stable Diffusion XL API. Generate images at 1024x1024 PNG format from prompts. Pin a specific SDXL model version for reproducibility. Handle Replicate cold starts (10-30s). Retry on failure with exponential backoff (max 3 retries). Track daily image generation count against the 50/day cap. Orchestrate the full flow: prompt generation, image generation, quality validation, post-processing, and upload to R2.","1. generate_image(prompt) sends a prompt to Replicate SDXL and returns the generated image as bytes; 2. Uses the pinned model version from REPLICATE_MODEL_VERSION environment variable; 3. Images are generated at 1024x1024 resolution in PNG format; 4. GIVEN Replicate returns an error WHEN generation retries 3 times with exponential backoff and all fail THEN the trend status is set to 'generation_failed' and an alert is sent; 5. GIVEN the daily image cap (50) has been reached WHEN a new trend is picked up THEN it remains in 'discovered' status and is queued for the next day; 6. Generates 3 images per trend (one per prompt from prompt_generator); 7. GIVEN all 3 images for a trend fail quality validation after retries THEN the trend status is set to 'quality_failed' and no stickers are created; 8. Each successful image is uploaded to R2 with a unique key; 9. Integration test verifies a test prompt returns a valid PNG from Replicate","STR-004,STR-005,STR-017",L,"backend,stickers,ai",To Do
STR-019,Implement image quality validator (src/stickers/quality_validator.py),Task,High,"Implement automated quality checks on generated images before post-processing. Validate: dimensions are 1024x1024, file size between 50KB-5MB, image has transparency (alpha channel), not mostly blank (<80% white/transparent pixels), aspect ratio after auto-crop between 0.5-2.0. On failure, trigger regeneration with modified prompt ('centered, simple composition') up to 2 retries.","1. validate_image(image_bytes) returns a result object with pass/fail status and failure reasons; 2. Rejects images not exactly 1024x1024; 3. Rejects images with file size < 50KB (likely blank) or > 5MB (too complex); 4. Checks for alpha channel presence after background removal; 5. GIVEN an image that is >80% white/transparent pixels after background removal WHEN validation runs THEN it is rejected with reason 'mostly_blank'; 6. GIVEN an image whose auto-cropped aspect ratio is outside 0.5-2.0 WHEN validation runs THEN it is rejected with reason 'bad_aspect_ratio'; 7. GIVEN a quality check fails WHEN retry is triggered THEN a modified prompt with 'centered, simple composition' appended is used (max 2 retries per image); 8. GIVEN all retries fail WHEN the validator finishes THEN the image is marked 'quality_failed'; 9. Unit tests verify each check with sample images (blank, oversized, valid, extreme aspect ratio)",STR-001,M,"backend,stickers,quality",To Do
STR-020,Implement image post-processor (src/stickers/post_processor.py),Task,High,"Implement Pillow-based image post-processing pipeline. Steps: remove/clean background (make transparent), auto-crop to content bounds, validate aspect ratio (0.5-2.0), resize to 900x900 print-ready at 300 DPI, generate 300x300 thumbnail, optimize file sizes (print-ready <2MB, thumbnail <500KB). Upload both versions to R2.","1. process_image(image_bytes) returns print-ready (900x900) and thumbnail (300x300) image bytes; 2. Background is cleaned/made transparent; 3. Image is auto-cropped to content bounds before resizing; 4. GIVEN a raw 1024x1024 PNG WHEN post-processing runs THEN the output is a 900x900 print-ready PNG with transparent background and a 300x300 thumbnail; 5. Print-ready image is optimized to < 2MB file size; 6. Thumbnail is optimized to < 500KB file size; 7. GIVEN an image that is mostly blank after background removal (>80% transparent) WHEN post-processing runs THEN it is rejected with status 'quality_failed'; 8. Both print-ready and thumbnail are uploaded to R2 with distinct keys; 9. Unit tests verify output dimensions, transparency, file size targets, and rejection of blank images","STR-001,STR-004",M,"backend,stickers,image-processing",To Do
STR-021,Implement content moderator (src/moderation/moderator.py),Task,High,"Implement the two-stage content moderation module. Stage 2 runs OpenAI Moderation API on generated images. Apply score thresholds: <0.4 auto-approve, 0.4-0.7 flag for manual review, >0.7 auto-reject. Check generated text/tags against keyword and trademark blocklists. Send email alerts for flagged content. Handle 48-hour auto-reject for unreviewed flagged stickers.","1. moderate_image(image_url, description, tags) calls OpenAI Moderation API and returns moderation_status (approved/flagged/rejected) and score; 2. GIVEN moderation score < 0.4 WHEN moderation runs THEN sticker is auto-approved with moderation_status='approved'; 3. GIVEN moderation score between 0.4 and 0.7 WHEN moderation runs THEN sticker is flagged with moderation_status='flagged' and an alert email is sent with image URL, topic, and score breakdown; 4. GIVEN moderation score > 0.7 WHEN moderation runs THEN sticker is auto-rejected with moderation_status='rejected' and reason logged; 5. GIVEN a sticker description contains a trademark blocklist term WHEN moderation runs THEN it is auto-rejected with reason='trademark_violation'; 6. GIVEN a flagged sticker has not been reviewed for 48 hours WHEN the daily analytics job runs THEN it is auto-rejected and an alert email is sent; 7. Moderation categories breakdown (JSONB) is stored on the sticker record; 8. Unit tests verify threshold logic, blocklist matching, and alert triggering","STR-005,STR-008,STR-015",M,"backend,moderation,ai",To Do
STR-022,Implement Etsy listing publisher (src/publisher/etsy.py),Task,Critical,"Implement the Etsy API v3 integration for creating and managing listings. Create draft listings, upload images, activate listings. Generate SEO-optimized titles (<=140 chars), descriptions (using spec template with AI disclosure), and 13 tags per listing. Set correct Etsy fields: who_made='someone_else', when_made='2020_2025', is_supply=false, shipping=$0.00. Enforce 300 active listing cap. Calculate and verify floor price before publishing.","1. create_listing(sticker) creates a new Etsy listing via API with title <= 140 chars, exactly 13 tags, price from pricing_tiers, shipping=$0.00, who_made='someone_else', when_made='2020_2025', is_supply=false, and AI disclosure in description; 2. GIVEN the active listing count is at 300 WHEN the publisher attempts to create a new listing THEN it skips publishing, logs the reason, and the sticker remains in 'approved' status; 3. GIVEN the Etsy API returns 429 (rate limit) WHEN the publisher encounters it THEN it stops publishing for this cycle and remaining items are retried next cycle; 4. GIVEN the Etsy OAuth token has expired WHEN publishing is attempted THEN the token is refreshed and the API call is retried; 5. GIVEN the calculated floor price exceeds the just_dropped tier price WHEN the publisher sets the price THEN it uses the floor price rounded up to the nearest .49 or .99; 6. Description follows the spec template including 'What You Get', 'Size and Material', 'Shipping', 'About This Design' (AI disclosure), and 'Shop Policies' sections; 7. Images are uploaded via Etsy API (sticker image + lifecycle mockups); 8. Listing metadata (trend_id, etsy_listing_id, price, shipping_cost, fulfillment_provider) is stored in Supabase; 9. Sticker status is updated to 'published' after successful listing creation; 10. update_listing_price(listing_id, new_price) updates an existing listing's price; 11. deactivate_listing(listing_id) deactivates a listing (for archiving)","STR-003,STR-004,STR-005,STR-009,STR-010",XL,"backend,publisher,etsy,api",To Do
STR-023,Implement lifestyle mockup generator (src/stickers/post_processor.py - mockup section),Task,Medium,"Implement Pillow-based composite rendering of sticker designs onto mockup templates. Composite sticker onto laptop template (assets/mockups/laptop_template.png) and water bottle template (assets/mockups/bottle_template.png) at appropriate positions and angles. These mockup images are uploaded as listing images 2 and 3 on Etsy.","1. generate_mockup(sticker_image, template_path, position, angle) composites the sticker onto the template and returns the result as PNG bytes; 2. Laptop mockup places the sticker at a predefined position on the laptop lid with appropriate sizing; 3. Water bottle mockup places the sticker at a predefined position on the bottle with appropriate sizing and slight curve distortion; 4. Output images are suitable for Etsy listing (high quality, realistic placement); 5. GIVEN a valid sticker image and laptop template WHEN generate_mockup runs THEN the output shows the sticker naturally placed on the laptop; 6. Mockup templates are loaded from assets/mockups/ directory; 7. Unit tests verify output image dimensions and that the sticker region is non-transparent in the composite","STR-001,STR-020",M,"backend,stickers,image-processing",To Do
STR-024,Implement Etsy SEO copy generator,Task,High,"Implement GPT-4o-mini-based generation of SEO-optimized listing copy for Etsy. Generate titles (<=140 chars following the format: '{Trend Topic} Sticker - {Style} Vinyl Decal - Laptop Water Bottle Sticker - Trending {Category}'). Generate 13 tags per listing (5-7 trend keywords, 3-4 evergreen sticker terms, 2-3 audience terms, always include 'free shipping'). Generate descriptions incorporating the spec template.","1. generate_title(trend_topic, style) returns a title <= 140 characters following the format from the spec; 2. generate_tags(trend_topic, keywords) returns exactly 13 tags with the required mix: 5-7 trend-specific, 3-4 evergreen ('vinyl sticker', 'laptop sticker', 'waterproof decal'), 2-3 audience terms ('funny sticker', 'meme sticker'), and always 'free shipping'; 3. generate_description(trend_topic, size, ai_description) returns the full listing description following the spec template with all sections; 4. GIVEN a trend about 'baby hippo' WHEN SEO copy is generated THEN the title includes the topic and is <= 140 chars, tags include both trend and evergreen terms, and description includes AI disclosure; 5. All generated text is checked against trademark blocklist before use; 6. Uses GPT-4o-mini with structured output for consistent formatting; 7. Unit tests verify title length, tag count, required tag categories, and description template sections","STR-005,STR-015",M,"backend,publisher,seo,ai",To Do
STR-025,Implement daily pricing engine (src/pricing/engine.py),Task,Critical,"Implement the daily pricing engine that runs at 6AM UTC via GitHub Actions. Adjusts sticker prices based on trend age: just_dropped (0-3 days), trending (4-14 days), cooling (15-30 days), evergreen (30+ days with sales), archived (30+ days no sales). Enforces sales override (10+ sales at current tier = keep price). Calculates floor prices. Updates Etsy listing prices via API. Logs all changes to price_history. Runs with Redis concurrency lock.","1. GIVEN a sticker linked to a 2-day-old trend WHEN the pricing engine runs THEN it is priced at 'just_dropped' tier ($5.49/$6.49 for 3in/4in); 2. GIVEN a sticker linked to a 10-day-old trend WHEN the pricing engine runs THEN it is priced at 'trending' tier ($4.49/$5.49); 3. GIVEN a sticker linked to a 20-day-old trend WHEN the pricing engine runs THEN it is priced at 'cooling' tier ($3.49/$4.49); 4. GIVEN a sticker linked to a 35-day-old trend with sales in the last 14 days WHEN the pricing engine runs THEN it is priced at 'evergreen' tier ($3.49/$4.49); 5. GIVEN a sticker with 0 sales and 0 views for 14+ days WHEN the pricing engine runs THEN the Etsy listing is deactivated (archived); 6. GIVEN a sticker has 10+ sales at the current pricing tier WHEN the engine checks THEN it keeps the current price (sales override); 7. GIVEN the new price would be below the floor price WHEN the engine calculates THEN it uses the floor price rounded to .49/.99; 8. All price changes are logged to price_history with old_price, new_price, tier, and reason; 9. Engine acquires Redis concurrency lock (TTL 30 min) before running; exits if lock is held; 10. Checks Etsy API rate limit budget; skips if > 8500 calls used today; 11. Sends daily summary email with repricing stats; 12. All prices end in .49 or .99","STR-003,STR-005,STR-009,STR-010,STR-022",XL,"backend,pricing,pipeline",To Do
STR-026,Implement pricing tier config and lookups (src/pricing/tiers.py),Task,Medium,"Implement the pricing tier configuration module that reads tier definitions from the pricing_tiers Supabase table and provides lookup functions. Calculate floor prices based on print cost, shipping cost, packaging cost, Etsy fee rate, and minimum margin target (20%).","1. get_tier_for_age(age_days) returns the correct pricing tier name based on trend age boundaries from the pricing_tiers table; 2. get_price(tier, product_type) returns the correct price for a given tier and product type (single_small or single_large); 3. calculate_floor_price(print_cost, shipping_cost, packaging_cost, etsy_fee_rate=0.10, min_margin=0.20) returns the floor price per the spec formula: (costs) / (1 - fee_rate) / (1 - margin); 4. round_to_price_point(price) rounds a price to the nearest .49 or .99; 5. GIVEN print_cost=$1.50, shipping=$0.93, packaging=$0.15 WHEN floor is calculated THEN result is approximately $3.59 rounded to $3.49; 6. GIVEN a price of $3.72 WHEN rounded THEN it becomes $3.99; 7. Unit tests verify tier boundaries, price lookups, floor calculation, and rounding","STR-003",S,"backend,pricing",To Do
STR-027,Implement sticker archiver (src/pricing/archiver.py),Task,Medium,"Implement the archiver that delists stale stickers to free up Etsy listing slots (max 300 active). A sticker is archived if it has 0 sales and 0 views for 14+ days. The archiver runs as part of the daily pricing engine before the publisher creates new listings. Deactivates listings via Etsy API and updates sticker status to 'archived'.","1. get_archivable_stickers() returns stickers with 0 sales and 0 views for 14+ days; 2. archive_sticker(sticker_id) deactivates the Etsy listing and sets sticker status to 'archived'; 3. GIVEN a sticker with 0 sales and 0 views for 14+ days WHEN the archiver runs THEN its Etsy listing is deactivated and status is set to 'archived'; 4. GIVEN a sticker with 1+ sales in the last 14 days WHEN the archiver runs THEN it is not archived; 5. The archiver runs before the publisher to free listing slots; 6. Archives are logged to price_history with reason='archived'; 7. Returns count of archived stickers for the daily summary email; 8. Unit tests verify archival criteria and edge cases (exactly 14 days, sales on day 13)","STR-003,STR-022",M,"backend,pricing,etsy",To Do
STR-028,Implement daily analytics sync (src/analytics/sync.py),Task,High,"Implement the daily analytics sync that runs at 8AM UTC. Syncs sales data from Etsy API (GET /shops/{shop_id}/receipts). Creates order records in Supabase. Updates sticker sales_count and view_count. Triggers order fulfillment. Auto-rejects unreviewed flagged stickers after 48 hours. Refreshes daily_metrics materialized view. Runs database backup. Sends daily summary email.","1. Fetches new orders from Etsy API and stores them in orders table with status='paid'; 2. Updates sticker sales_count and view_count from Etsy listing stats; 3. GIVEN a new order from Etsy WHEN the sync runs THEN an order record is created with etsy_order_id, sticker_id, quantity, unit_price, total_amount, and pricing_tier_at_sale; 4. Triggers fulfillment for new orders (calls fulfillment router); 5. GIVEN a flagged sticker has not been reviewed for 48 hours WHEN the sync runs THEN it is auto-rejected and an alert email is sent; 6. Refreshes the daily_metrics materialized view; 7. Runs PII purge: sets customer_data=NULL on orders delivered 90+ days ago; 8. Acquires Redis concurrency lock (TTL 30 min); exits if lock is held; 9. Sends daily summary email with all sections from the spec template; 10. Logs run to pipeline_runs table","STR-003,STR-005,STR-006,STR-007,STR-008,STR-009,STR-010",L,"backend,analytics,pipeline",To Do
STR-029,Implement daily metrics aggregation (src/analytics/metrics.py),Task,Medium,"Implement the metrics aggregation module that computes daily business metrics: orders count, gross revenue, COGS, Etsy fees, estimated profit, new listings count, average order value. Supports the daily summary email and cost tracking view.","1. get_daily_metrics(date) returns metrics for a given date: orders, gross_revenue, cogs, etsy_fees, estimated_profit, new_listings, avg_order_value; 2. get_mtd_metrics() returns month-to-date aggregates; 3. get_ai_spend(date) returns total AI cost for the day from pipeline_runs.ai_cost_estimate_usd; 4. get_api_usage(date) returns Etsy API calls used from pipeline_runs.etsy_api_calls_used; 5. refresh_materialized_view() refreshes the daily_metrics materialized view; 6. GIVEN 5 orders on a date WHEN get_daily_metrics is called THEN it returns correct revenue, COGS, and profit calculations; 7. Unit tests verify calculations against known test data","STR-003,STR-006",S,"backend,analytics",To Do
STR-030,Implement fulfillment router (src/fulfillment/router.py),Task,High,"Implement the fulfillment routing logic. Primary provider: Sticker Mule. Fallback: self-fulfillment (USPS). Route orders to the appropriate provider based on availability. Handle provider failures with retry and fallback. MVP routes singles only (no packs).","1. route_order(order) determines the fulfillment provider (primary: sticker_mule, fallback: self_usps) and returns the provider name; 2. fulfill_order(order) sends the print job to the selected provider with image URL, shipping address, sticker size, and quantity; 3. GIVEN Sticker Mule API is available WHEN an order is routed THEN it goes to Sticker Mule; 4. GIVEN Sticker Mule API fails after 3 retries WHEN the router processes the order THEN it routes to self-fulfillment, sends an alert email ('Order {etsy_order_id} needs manual fulfillment'), and sets order status to 'pending_manual'; 5. Updates order status to 'sent_to_print' on successful submission; 6. Stores fulfillment_provider and fulfillment_order_id on the order record; 7. Tracks fulfillment_attempts and fulfillment_last_error for debugging; 8. Unit tests verify routing logic, fallback behavior, and status updates","STR-003,STR-005,STR-008",M,"backend,fulfillment",To Do
STR-031,Implement Sticker Mule API client (src/fulfillment/sticker_mule.py),Task,High,"Implement the Sticker Mule API client for submitting on-demand print orders. Submit print jobs with image URL, shipping address, sticker size, quantity. Query order status for shipping updates. Handle API errors and rate limits.","1. submit_order(image_url, address, size, quantity) submits a print order to Sticker Mule and returns an order ID; 2. get_order_status(order_id) returns the current status of a Sticker Mule order (processing, shipped, delivered); 3. GIVEN valid order details WHEN submit_order is called THEN a print order is created with Sticker Mule and an order ID is returned; 4. GIVEN Sticker Mule returns an error WHEN submit_order is called THEN the error is caught, logged with retry_count, and raised for retry handling; 5. get_tracking_number(order_id) returns the tracking number when available; 6. Authenticates with STICKER_MULE_API_KEY; 7. Integration test verifies API connectivity (if sandbox available)","STR-001,STR-005",M,"backend,fulfillment,api",To Do
STR-032,Implement self-fulfillment order tracker (src/fulfillment/self_fulfill.py),Task,Medium,"Implement the self-fulfillment tracking module for orders fulfilled via USPS. Creates a record of orders that need manual fulfillment (print, package, ship). Provides a simple interface to update order status as the operator processes them. Sends alert emails for orders needing manual attention.","1. create_self_fulfillment_order(order) records the order as needing manual fulfillment with all necessary details (image URL, shipping address, size, quantity); 2. Sends an alert email to the operator with order details and fulfillment instructions; 3. GIVEN an order is routed to self-fulfillment WHEN the tracker creates the record THEN the order status is set to 'pending_manual' and fulfillment_provider is 'self_usps'; 4. get_pending_orders() returns all orders in 'pending_manual' status; 5. mark_shipped(order_id, tracking_number) updates the order status to 'shipped' with tracking info; 6. GIVEN an order has been in 'sent_to_print' for 7+ days with no shipping update WHEN the daily sync checks THEN an alert email is sent; 7. Unit tests verify status transitions and alert triggering","STR-003,STR-008",S,"backend,fulfillment",To Do
STR-033,Implement daily database backup (src/backup/db_backup.py),Task,Medium,"Implement the daily database backup module that runs as part of the daily analytics workflow. Export Supabase database via pg_dump, compress with gzip, upload to R2 at backups/db/sticker-trendz-{YYYY-MM-DD}.sql.gz. Delete backups older than 30 days. Log success/failure to pipeline_runs.","1. run_backup() exports the Supabase database, compresses it, and uploads to R2; 2. Backup file is named backups/db/sticker-trendz-{YYYY-MM-DD}.sql.gz; 3. Backups older than 30 days are automatically deleted from R2; 4. GIVEN the backup succeeds WHEN run_backup completes THEN success is logged to pipeline_runs; 5. GIVEN the backup fails WHEN an error occurs THEN the failure is logged to error_log and an alert is sent; 6. GIVEN there are 35 backups in R2 WHEN cleanup runs THEN the 5 oldest are deleted; 7. Backup uses the Supabase service role connection string","STR-003,STR-004,STR-006",M,"backend,backup,infrastructure",To Do
STR-034,Create GitHub Actions workflow: trend-monitor.yml,Task,Critical,"Create the trend-monitor.yml GitHub Actions workflow per the spec. Cron schedule: every 2 hours. Two jobs: monitor (always runs) and generate (conditional on new_trends output). Configure pip caching, Python 3.12, all required secrets and variables. Set concurrency group to prevent overlapping runs.","1. Workflow triggers on cron '0 */2 * * *' and workflow_dispatch (manual trigger); 2. Concurrency group 'trend-monitor' with cancel-in-progress=false; 3. Monitor job runs on ubuntu-latest with Python 3.12 and pip cache; 4. Monitor job outputs new_trends variable from the detect step; 5. Generate job has 'needs: monitor' and 'if: needs.monitor.outputs.new_trends == true'; 6. All secrets are passed as environment variables per the spec (OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_KEY, etc.); 7. MAX_TRENDS_PER_CYCLE and MAX_IMAGES_PER_DAY use vars with defaults of 5 and 50; 8. Workflow matches the exact YAML structure from the spec","STR-016,STR-018",M,"devops,ci-cd,github-actions",To Do
STR-035,Create GitHub Actions workflow: daily-pricing.yml,Task,High,"Create the daily-pricing.yml GitHub Actions workflow per the spec. Cron schedule: daily at 6AM UTC. Runs the pricing engine. Configure pip caching, Python 3.12, all required secrets. Set concurrency group to prevent overlapping runs.","1. Workflow triggers on cron '0 6 * * *' and workflow_dispatch; 2. Concurrency group 'pricing-engine' with cancel-in-progress=false; 3. Runs on ubuntu-latest with Python 3.12 and pip cache; 4. Passes all required secrets: SUPABASE_URL, SUPABASE_SERVICE_KEY, ETSY_API_KEY, ETSY_API_SECRET, ETSY_SHOP_ID, UPSTASH_REDIS_URL, UPSTASH_REDIS_TOKEN, SENDGRID_API_KEY, ALERT_EMAIL; 5. Executes 'python -m src.pricing.engine'; 6. Workflow matches the spec YAML structure",STR-025,S,"devops,ci-cd,github-actions",To Do
STR-036,Create GitHub Actions workflow: daily-analytics.yml,Task,High,"Create the daily-analytics.yml GitHub Actions workflow per the spec. Cron schedule: daily at 8AM UTC. Runs analytics sync followed by database backup. Configure pip caching, Python 3.12, all required secrets.","1. Workflow triggers on cron '0 8 * * *' and workflow_dispatch; 2. Concurrency group 'analytics' with cancel-in-progress=false; 3. Runs on ubuntu-latest with Python 3.12 and pip cache; 4. Passes all required secrets including STICKER_MULE_API_KEY for fulfillment; 5. Executes 'python -m src.analytics.sync' followed by 'python -m src.backup.db_backup' as separate steps; 6. Workflow matches the spec YAML structure",STR-028,S,"devops,ci-cd,github-actions",To Do
STR-037,Implement PII purge job for data retention compliance,Task,Medium,"Implement the PII purge job as part of the daily analytics sync. Sets customer_data=NULL on orders that were delivered 90+ days ago. Also deletes error_log entries older than 90 days and pipeline_runs entries older than 180 days. Archives price_history entries older than 1 year to R2 as CSV before deleting.","1. purge_pii() sets customer_data=NULL on orders WHERE status='delivered' AND delivered_at < NOW() - INTERVAL '90 days' AND customer_data IS NOT NULL; 2. purge_error_logs() deletes error_log entries older than 90 days; 3. purge_pipeline_runs() deletes pipeline_runs entries older than 180 days; 4. archive_price_history() exports price_history entries older than 1 year to CSV, uploads to R2, then deletes from database; 5. GIVEN an order was delivered 91 days ago WHEN the purge runs THEN its customer_data is set to NULL but the order record itself is preserved; 6. GIVEN an order was delivered 89 days ago WHEN the purge runs THEN its customer_data is NOT modified; 7. Logs the count of purged records for audit; 8. Unit tests verify purge criteria and edge cases","STR-003,STR-004",S,"backend,security,compliance",To Do
STR-038,Implement AI spend tracking and budget enforcement,Task,High,"Implement cost tracking for AI API usage. Estimate OpenAI costs per request (GPT-4o-mini: $0.15/1M input, $0.60/1M output tokens). Estimate Replicate costs per image (~$0.02-0.05/image). Track cumulative daily and monthly spend. Alert at $120/month. Hard stop at $150/month. Log cost estimates in pipeline_runs.ai_cost_estimate_usd.","1. estimate_openai_cost(input_tokens, output_tokens) returns the estimated cost in USD using GPT-4o-mini pricing; 2. estimate_replicate_cost(image_count) returns estimated cost based on configured per-image rate; 3. get_monthly_spend() sums ai_cost_estimate_usd from pipeline_runs for the current month; 4. check_budget() returns whether operations can proceed based on monthly spend vs. $150 cap; 5. GIVEN monthly AI spend exceeds $120 WHEN check_budget runs THEN a warning alert email is sent; 6. GIVEN monthly AI spend exceeds $150 WHEN check_budget runs THEN all AI operations are halted with a hard stop alert; 7. Cost estimates are logged in pipeline_runs.ai_cost_estimate_usd for each run; 8. GIVEN daily AI spend exceeds $8 WHEN the daily check runs THEN a warning is triggered; 9. Unit tests verify cost calculations and threshold logic","STR-003,STR-006,STR-008",M,"backend,monitoring,cost",To Do
STR-039,Write unit tests for trend scorer,Task,High,"Implement test_scorer.py with comprehensive tests for the GPT-4o-mini trend scoring module. Test score parsing, threshold filtering (7.0+), retry on malformed JSON, prompt construction, and edge cases. Use mocked OpenAI API responses.","1. Test that valid JSON response is correctly parsed into score object with all fields; 2. Test that trends scoring >= 7.0 overall are returned as qualifying; 3. Test that trends scoring < 7.0 overall are filtered out; 4. Test that malformed JSON triggers retry (up to 2 retries); 5. Test that after 3 failed JSON parses the trend is skipped and error is logged; 6. Test prompt includes all required elements (system message, calibration examples, trend data); 7. Test score field validation (integers 1-10, float 1.0-10.0); 8. All tests use mocked OpenAI client (no real API calls); 9. At least 8 test cases covering happy path and edge cases",STR-014,M,"testing,unit-test,trends",To Do
STR-040,Write unit tests for pricing engine,Task,High,"Implement test_pricing_engine.py and test_floor_price.py with comprehensive tests for pricing tier assignment, price lookups, floor price calculation, sales override logic, and archival criteria.","1. Test tier assignment: 2 days=just_dropped, 10 days=trending, 20 days=cooling, 35 days with sales=evergreen; 2. Test correct prices are returned for each tier and product type; 3. Test sales override: 10+ sales at current tier keeps price unchanged; 4. Test sales override counter resets when tier changes; 5. Test floor price calculation: (costs)/(1-fee_rate)/(1-margin) with known inputs; 6. Test price never goes below floor; 7. Test price rounding to .49 or .99; 8. Test archival: 0 sales and 0 views for 14+ days triggers archive; 9. Test that sticker with recent sales is NOT archived; 10. At least 15 test cases across both files","STR-025,STR-026",M,"testing,unit-test,pricing",To Do
STR-041,Write unit tests for post-processor and quality validator,Task,Medium,"Implement test_post_processor.py and test_quality_validator.py with tests for image processing and quality validation. Test dimension checks, transparency, blank detection, aspect ratio, file size, and resize operations. Use sample test images.","1. Test that 1024x1024 input produces 900x900 print-ready and 300x300 thumbnail outputs; 2. Test that output images have alpha channel (transparency); 3. Test that mostly blank image (>80% transparent) is rejected; 4. Test that extreme aspect ratio (outside 0.5-2.0) after crop is rejected; 5. Test that file size < 50KB is rejected; 6. Test that file size > 5MB is rejected; 7. Test that valid image passes all quality checks; 8. Test that print-ready output is < 2MB; 9. Test that thumbnail output is < 500KB; 10. At least 10 test cases with sample images","STR-019,STR-020",M,"testing,unit-test,stickers",To Do
STR-042,Write unit tests for deduplication and rate limiter,Task,Medium,"Implement test_dedup.py and test_rate_limiter.py with tests for Jaccard similarity, topic normalization, merge logic, Redis rate limit tracking, and priority-based throttling.","1. Test Jaccard similarity calculation with known sets (e.g., {a,b,c} vs {b,c,d} = 0.5); 2. Test that similarity > 0.6 triggers merge; 3. Test that similarity <= 0.6 keeps trends separate; 4. Test topic normalization (lowercase, stemming); 5. Test merge correctly combines sources arrays; 6. Test rate limiter: 0-7000 calls allows all priorities; 7. Test rate limiter: 7001-8500 blocks P3 only; 8. Test rate limiter: 8501-9500 blocks P2 and P3; 9. Test rate limiter: 9501+ blocks all operations; 10. Test Redis lock acquire/release; 11. At least 12 test cases across both files","STR-013,STR-009",M,"testing,unit-test",To Do
STR-043,Write unit tests for trademark blocklist,Task,Medium,"Implement test_trademark_blocklist.py with tests for trademark and keyword blocklist matching. Verify known brands are blocked, clean topics pass, and edge cases are handled (partial matches, case insensitivity, plurals).","1. Test that 'Mickey Mouse' is detected as trademark; 2. Test that 'Nike shoes' is detected as trademark; 3. Test that 'baby hippo' passes trademark check; 4. Test case insensitivity: 'DISNEY' matches 'Disney'; 5. Test partial match: 'Spider-Man costume' matches 'Spider-Man'; 6. Test keyword blocklist blocks offensive terms; 7. Test clean topics pass keyword blocklist; 8. Test edge case: empty string input; 9. Test edge case: very long input string; 10. At least 10 test cases",STR-015,S,"testing,unit-test,moderation",To Do
STR-044,Write integration tests for external APIs,Task,Medium,"Implement integration tests: test_reddit_source.py, test_google_trends_source.py, test_replicate_gen.py, test_etsy_sandbox.py, test_supabase_crud.py, test_r2_upload.py. Each test verifies real API connectivity and data parsing. These run on-demand (not on every PR).","1. test_reddit_source.py: verifies Reddit OAuth returns parseable trending posts from at least one subreddit; 2. test_google_trends_source.py: verifies pytrends returns breakout terms; 3. test_replicate_gen.py: verifies a simple test prompt returns a valid PNG image; 4. test_etsy_sandbox.py: verifies listing creation works in Etsy sandbox (create, upload image, activate, deactivate); 5. test_supabase_crud.py: verifies insert, select, update, delete on all major tables; 6. test_r2_upload.py: verifies image upload and retrieval from R2; 7. Each test is skipped gracefully if API credentials are not configured; 8. Tests are marked with @pytest.mark.integration so they can be run separately","STR-011,STR-012,STR-018,STR-022,STR-003,STR-004",L,"testing,integration-test",To Do
STR-045,Write end-to-end pipeline tests,Task,Medium,"Implement test_full_pipeline.py and test_pricing_cycle.py. Full pipeline test injects a fake trend and verifies it flows through scoring, generation, moderation, and publishing (Etsy sandbox). Pricing cycle test seeds stickers with known ages and verifies tier assignments and price changes.","1. test_full_pipeline.py: injects a fake trend with known topic and verifies it progresses through discovered -> generation -> moderation -> published; 2. Asserts the Etsy listing (sandbox) has correct title format, exactly 13 tags, correct pricing tier, and uploaded images; 3. test_pricing_cycle.py: seeds test stickers with ages of 2, 10, 20, and 35 days; 4. Runs pricing engine and verifies tier assignments (just_dropped, trending, cooling, evergreen/archived); 5. Verifies price changes are logged to price_history; 6. Verifies archived stickers have deactivated listings; 7. Tests are marked with @pytest.mark.e2e; 8. Tests use Etsy sandbox environment","STR-016,STR-018,STR-025,STR-022",L,"testing,e2e-test",To Do
STR-046,Set up Etsy shop configuration and sections,Task,Medium,"Configure the Etsy shop with the required sections defined in the spec: 'Trending Now' (just_dropped tier), 'Popular' (5+ sales), 'New Drops' (last 7 days), 'Under $5' (cooling/evergreen tier). Set up the shop About page with AI disclosure, production process overview, and shipping info. Configure US-only shipping profile with free shipping. Look up and store the Stickers taxonomy_id.","1. Four shop sections are created: 'Trending Now', 'Popular', 'New Drops', 'Under $5' with correct sort orders per spec; 2. Shop About page includes AI disclosure, production process, and shipping info; 3. US-only shipping profile is configured with $0.00 shipping (free); 4. Stickers taxonomy_id is looked up via Etsy API and stored in config; 5. Privacy policy link is added to the shop; 6. who_made and when_made defaults are verified to work with Etsy for AI-generated products; 7. At least 10 test listings are created in Etsy sandbox to verify all fields render correctly","STR-010,STR-022",M,"setup,etsy,configuration",To Do
STR-047,Create mockup template assets,Task,Low,"Create or source the static mockup templates for lifestyle images: laptop_template.png and bottle_template.png. Store in assets/mockups/ directory. Define the sticker placement coordinates and sizing for each template.","1. assets/mockups/laptop_template.png exists and is a high-quality laptop image with a clear area for sticker placement; 2. assets/mockups/bottle_template.png exists and is a high-quality water bottle image with a clear area for sticker placement; 3. A configuration file or constants define the sticker placement coordinates, size, and angle for each template; 4. Templates are suitable for Etsy listing images (professional quality, good lighting); 5. Templates are free/licensed for commercial use",None,S,"assets,design",To Do
STR-048,Implement Etsy listing section management,Task,Low,"Implement logic to assign published stickers to the correct Etsy shop sections based on their pricing tier and sales count. 'Trending Now' for just_dropped tier, 'Popular' for 5+ sales, 'New Drops' for last 7 days, 'Under $5' for cooling/evergreen tier. Update sections as stickers change tiers.","1. assign_section(sticker) determines the correct shop section based on pricing tier and sales count; 2. GIVEN a just_dropped sticker WHEN published THEN it is assigned to 'Trending Now' section; 3. GIVEN a sticker with 5+ sales WHEN sections are updated THEN it is moved to 'Popular' section; 4. GIVEN a cooling/evergreen sticker WHEN sections are updated THEN it is assigned to 'Under $5' section; 5. Section assignments are updated as part of the daily pricing engine run; 6. Uses Etsy API to manage section assignments; 7. Unit tests verify section assignment logic","STR-022,STR-025,STR-046",S,"backend,etsy",To Do
